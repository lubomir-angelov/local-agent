services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen25-coder-32b-awq
    restart: unless-stopped
      

    # GPU access
    gpus: all

    # vLLM strongly benefits from shared memory; their Docker docs recommend ipc=host or shm-size. :contentReference[oaicite:4]{index=4}
    ipc: host

    ports:
      - "${VLLM_PORT:-8000}:8000"

    # Persist model downloads
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
      - "${HOME}/.cache/torch_extensions:/root/.cache/torch_extensions"

    environment:
      - HF_TOKEN=${HF_TOKEN}
      - TORCH_EXTENSIONS_DIR=/root/.cache/torch_extensions
      - TORCH_CUDA_ARCH_LIST=12.0

    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen2.5-Coder-32B-Instruct-AWQ"
      - "--served-model-name"
      - "qwen2.5-coder-32b-awq"
      - "--dtype"
      - "half"
      - "--max-model-len"
      - "${VLLM_MAX_MODEL_LEN:-16384}"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--api-key"
      - "${VLLM_API_KEY}"

